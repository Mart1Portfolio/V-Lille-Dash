name: Get VLille info every 10min by excecuting a Python Script and saving CSV

on:
  schedule:
    - cron: '*/10 * * * *'  # Every 10 min
  workflow_dispatch:  

jobs:
  run-script:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout the repository
        uses: actions/checkout@v4
        with : 
          fetch-depth: 1 #Permet d'éviter de récupérer tout l'historique 

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.13'
      - name: Configure GCP credentials
        run: |
          # 1. Prend le Secret GitHub et le décode dans un fichier
          echo "${{ secrets.DASH_VLILLE_BQ_PUSHER }}" > gcp_key.json
          
          # 2. Définit une variable d'environnement pour que pandas-gbq
          # trouve et utilise automatiquement ce fichier.
          echo "GOOGLE_APPLICATION_CREDENTIALS=$GITHUB_WORKSPACE/gcp_key.json" >> $GITHUB_ENV
      - name: Install Scraper Dependencies
        run: |
          pip install uv  # Installer uv via pip
          cd scrape_data  # 1. On se déplace dans le dossier du scraper
          uv sync         # 2. On installe ses dépendances (via uv.lock)
      - name: Run Python script with uv
        run: |
          cd scrape_data  # Se déplacer dans le dossier du scraper
          uv run scrapper.py  # Execute the script

        env:
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
          BIGQUERY_DATASET_ID: ${{ secrets.BIGQUERY_DATASET_ID }}
          BIGQUERY_TABLE_ID: ${{ secrets.BIGQUERY_TABLE_ID }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}  
